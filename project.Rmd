---
title: "Course Project"
author: "ahs"
date: "December 14, 2014"
output: html_document
---
# Project Goal

The goal of this project is to determine how well people perform a particular exercise. I have no idea why all these errors/warnings are popping up here, so sorry. 

```{r echo=FALSE, warning=FALSE, error=FALSE}
# Load libraries
library(caret)
library(lattice)
library(ggplot2)
library(rattle)
library(dplyr)
library(randomForest)

# Load the data
train_data <- read.csv('pml-training.csv')


```

  
# Exploration

The first step is to figure out what columns to use or not. We start with about 160 columns, which is a lot. I found that many columns weren't well populated, so I went through and removed those. To do this, I kept a data structure of which columns had good values for use in cleaning the test data. 

```{r echo=FALSE, warning=FALSE, error=FALSE}
# 
# # Figure out if any column is all NAs
# sapply(train_data, function(x)all(is.na(x)))
# 
# # It's not, so figure out how many columns are MOSTLY NAs. 
# sapply(train_data, 
#        function(x)NROW(na.omit(x))/NROW(x))
# 
# # Looking at the results, we see that all columns have either all measures 
# #   or the same number of non-measures. Let's get rid of the columns 
# #   that have 2% of values. 
# sapply(train_data, 
#        function(x)NROW(na.omit(x)))

# Turn it into a logical expression so we can use it for filtering
keep_cols <- sapply(train_data, 
       function(x)NROW(na.omit(x))!=406)

train_data <- train_data[keep_cols]

# We still have a bunch of columns with empty values, because 
# it sees "" as a factor level. I think these also have "#DIV/0" as a factor 
# level. Let's see if we can figure out which columns can be removed based
# on this. 

# We have a couple columns we need to fix. 
train_data$cvtd_timestamp <- as.character(train_data$cvtd_timestamp)
# I'm not quite sure if this is how i want to handle this, but it's what it is for now. 
train_data$new_window <- as.character(train_data$new_window)

keep_cols_2 <- sapply(train_data, function(x)!any(levels(x)=='#DIV/0!'))
train_data <- train_data[keep_cols_2]
      


```

Now we have `r ncol(train_data)` columns of data. 

```{r echo=FALSE, warning=FALSE, error=FALSE}
train_data <- train_data[2:60]

# modFit <- train(classe ~ .,method="rpart", data=train_data[1:2000,])
# print(modFit$finalModel)
# plot(modFit$finalModel, uniform=TRUE, main="Classification Tree")
# text(modFit$finalModel, use.n=TRUE, all=TRUE, cex=.8)
# fancyRpartPlot(modFit$finalModel)

```

Looking at the data, I thought it might be helpful to have a notion of where in the window the sample was taken, so I generated a t2_delta column to account for this. I haven't yet evaluated if this was at all meaningful. 


```{r  echo=FALSE, warning=FALSE, error=FALSE}

train_data$win_ind <- ave(train_data$new_window=="yes", FUN = function(x) {
            cumsum(x) })

# foo <- data.frame(diff(train_data$raw_timestamp_part_2))
# foo<- rbind(0,foo)
# colnames(foo) <- c("t2_delta")
# foo[foo$some_diff < 0,] <- 0

# Another way to do this, but it creates a group, which I may or may not want. 
train_data <- group_by(train_data, num_window) %>% mutate(t2_delta = c(0,diff(raw_timestamp_part_2)))




```

After building a classification tree that over-relied on the character columns of user name and such, I removed those, then built a new model. 


```{r echo=FALSE, warning=FALSE, error=FALSE}

# Okay, building a ctree again. 
train_data <- train_data[6:61]

#modFit <- train(classe ~ .,method="rpart", data=train_data[,-55]) # excluding "win_ind"
#save(modFit, file="myClassTree.RData")

# Instead of training again, just load the saved one from file. 
load("myClassTree.RData")

print(modFit$finalModel)
plot(modFit$finalModel, uniform=TRUE, main="Classification Tree")
text(modFit$finalModel, use.n=TRUE, all=TRUE, cex=.8)
fancyRpartPlot(modFit$finalModel)


```

This looks more interesting than previous, so I'm going with this for now. 

Now, let's generate a random forest. Because it was taking so long to build, I created a new subset of train data to train faster. 

```{r warning=FALSE, error=FALSE}

in_small_train <- createDataPartition(y=train_data$classe,p=0.2, list=FALSE)
small_td <- train_data[in_small_train,]

#modFit_rf <- train(classe~.,data=small_td,method="rf",prox=TRUE)
#save(modFit_rf, file="myRandomForest.RData")

load(file="myRandomForest.RData")


```

Evaluating the result on the small set of data we trained with: 

```{r echo=FALSE, warning=FALSE, error=FALSE}
getTree(modFit_rf$finalModel,k=2)
pred_train <- predict(modFit_rf, small_td)
small_td$predRight <- pred_train==small_td$classe
table(pred_train, small_td$classe)

```

That looks good, so I'm going to apply it to the entire training set: 

```{r echo=FALSE, warning=FALSE, error=FALSE}
pred_train2 <- predict(modFit_rf, train_data)
train_data$predRight <- pred_train2==train_data$classe
table(pred_train2, train_data$classe)


```


Since that looks good too, I'm going to go ahead and apply that to the test data. I created a function using my saved columns above to apply to the test data. 

```{r warning=FALSE, error=FALSE}

clean_test_data <- function(data){
  
  data <- data[keep_cols]
  data <- data[keep_cols_2]
  
  # Get rid of X column
  data <- data[2:60]
  
  
  # Add the time diff column (t2_delta)
  data <- group_by(data, num_window) %>% mutate(t2_delta = 
                                                  c(0,diff(raw_timestamp_part_2)))
  
   data$win_ind <- ave(data$new_window=="yes", FUN = function(x) {
            cumsum(x) })
  
  # Get rid of other cols
  data <- data[6:61]
  
 
  # Remove win_ind col

}

```

# Testing

Clearly, something is wrong with my model because I predict that everything is A. 

```{r echo=FALSE, warning=FALSE, error=FALSE}
testing<- read.csv('pml-testing.csv')

# Repeat the transforms, which are captured in a function. 

clean_test <- clean_test_data(testing)

pred_test <- predict(modFit_rf, clean_test)
pred_test

```


