{
    "contents" : "---\ntitle: \"Course Project\"\nauthor: \"ahs\"\ndate: \"December 14, 2014\"\noutput: pdf_document\n---\nProject Goal\nThe goal of this project is to determine how well people perform a particular exercise. \n\n```{r}\n# Load libraries\nlibrary(caret)\nlibrary(rattle)\nlibrary(dplyr)\n\n# Load the data\ntrain_data <- read.csv('pml-training.csv')\n\n\n```\n\n1. Load the training data. \n2. Do some exploration. \n    + PCA to figure out what variables to get rid of. \n    + \n3. Decide on a model\n  + Regression/linear model\n  + Classification tree\n  + Random forest\n  \n# Exploration\nHow to deal with NA values\n\n# PCA\n\n```{r}\nprComp <- prcomp(na.omit(train_data[,sapply(train_data,is.numeric)]))\nsummary(prComp)\nplot(prComp, type=\"lines\")\n\n```\n\n\n# Classification Tree\n\n```{r}\nmodFit <- train(classe ~ .,method=\"rpart\", data=train_data)\nprint(modFit$finalModel)\nplot(modFit$finalModel, uniform=TRUE, main=\"Classification Tree\")\ntext(modFit$finalModel, use.n=TRUE, all=TRUE, cex=.8)\nfancyRpartPlot(modFit$finalModel)\n\n```\n\nAwesome. See that amazing classification tree? That shows us that column X is the best predictor for classe. Yep, it's true! \n\nClearly, to train the model, we will need to remove X from the training set. But first, let's get rid of columns that are mostly NA. \n\n\n```{r}\n# Figure out if any column is all NAs\nsapply(train_data, function(x)all(is.na(x)))\n\n# It's not, so figure out how many columns are MOSTLY NAs. \nsapply(train_data, \n       function(x)NROW(na.omit(x))/NROW(x))\n\n# Looking at the results, we see that all columns have either all measures \n#   or the same number of non-measures. Let's get rid of the columns \n#   that have 2% of values. \nsapply(train_data, \n       function(x)NROW(na.omit(x)))\n\n# Turn it into a logical expression so we can use it for filtering\nkeep_cols <- sapply(train_data, \n       function(x)NROW(na.omit(x))!=406)\n\ntrain_data <- train_data[keep_cols]\n\n# We still have a bunch of columns with empty values, because \n# it sees \"\" as a factor level. I think these also have \"#DIV/0\" as a factor \n# level. Let's see if we can figure out which columns can be removed based\n# on this. \n\n# We have a couple columns we need to fix. \ntrain_data$cvtd_timestamp <- as.character(train_data$cvtd_timestamp)\n# I'm not quite sure if this is how i want to handle this, but it's what it is for now. \ntrain_data$new_window <- as.character(train_data$new_window)\n\nkeep_cols_2 <- sapply(train_data, function(x)!any(levels(x)=='#DIV/0!'))\ntrain_data <- train_data[keep_cols_2]\n      \n\n\n\n```\n\nNow we have `r ncol(train_data)` columns of data. \n\n```{r}\ntrain_data <- train_data[2:60]\n\nmodFit <- train(classe ~ .,method=\"rpart\", data=train_data[1:2000,])\nprint(modFit$finalModel)\nplot(modFit$finalModel, uniform=TRUE, main=\"Classification Tree\")\ntext(modFit$finalModel, use.n=TRUE, all=TRUE, cex=.8)\nfancyRpartPlot(modFit$finalModel)\n\n```\n\n```{r}\n\nprComp <- prcomp(na.omit(train_data[,sapply(train_data,is.numeric)]))\n\nprComp <- prcomp(train_data)\nsummary(prComp)\nplot(prComp, type=\"lines\")\n\n```\n\n```{r}\n\ntransform(train_data,\n          win_ind = ave(new_window==\"yes\", FUN = function(x) {\n            cumsum(x) }))\n\ntrain_data$win_ind <- ave(train_data$new_window==\"yes\", FUN = function(x) {\n            cumsum(x) })\n\nfoo <- data.frame(diff(train_data$raw_timestamp_part_2))\nfoo<- rbind(0,foo)\ncolnames(foo) <- c(\"t2_delta\")\nfoo[foo$some_diff < 0,] <- 0\n\n# Another way to do this, but it creates a group, which I may or may not want. \n#train_data <- group_by(train_data, num_window) %>% mutate(t2_delta = c(0,diff(raw_timestamp_part_2)))\n\n\n```\n\n\n```{r}\n\n# Okay, building a ctree again. \ntrain_data <- train_data[6:61]\n\nmodFit <- train(classe ~ .,method=\"rpart\", data=train_data[,-55]) # excluding \"win_ind\"\nprint(modFit$finalModel)\nplot(modFit$finalModel, uniform=TRUE, main=\"Classification Tree\")\ntext(modFit$finalModel, use.n=TRUE, all=TRUE, cex=.8)\nfancyRpartPlot(modFit$finalModel)\n\n\n```\n\n\nEnough of this, let's generate a random forest. \n\n```{r}\n\nin_small_train <- createDataPartition(y=train_data$classe,p=0.2, list=FALSE)#train_data[1:2000]\nsmall_td <- train_data[in_small_train,]\nmodFit_rf <- train(classe~.,data=small_td,method=\"rf\",prox=TRUE)\n\n```\n\nEvaluating the result on the data we trained with: \n\n```{r}\ngetTree(modFit_rf$finalModel,k=2)\nfancyRpartPlot(modFit_rf$finalModel)\npred_train <- predict(modFit_rf, small_td)\nsmall_td$predRight <- pred_train==small_td$classe\ntable(pred_train, small_td$classe)\n\npred_train2 <- predict(modFit_rf, train_data)\ntrain_data$predRight <- pred_train2==train_data$classe\ntable(pred_train2, train_data$classe)\n\n\n```\n\n\n```{r}\n\nclean_test_data <- function(data){\n  \n  data <- data[keep_cols]\n  data <- data[keep_cols_2]\n  \n  # Get rid of X column\n  data <- data[2:60]\n  \n  \n  # Add the time diff column (t2_delta)\n  data <- group_by(data, num_window) %>% mutate(t2_delta = \n                                                  c(0,diff(raw_timestamp_part_2)))\n  \n   data$win_ind <- ave(data$new_window==\"yes\", FUN = function(x) {\n            cumsum(x) })\n  \n  # Get rid of other cols\n  data <- data[6:61]\n  \n \n  # Remove win_ind col\n\n}\n\n```\n\n# Testing\n\n```{r}\ntesting<- read.csv('pml-testing.csv')\n\n# Repeat the transforms, which are captured in a function. \n\nclean_test <- clean_test_data(testing)\n\npred_test <- predict(modFit_rf, clean_test)\n\n\n```\n\nThis is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see <http://rmarkdown.rstudio.com>.\n\nWhen you click the **Knit** button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this:\n\n```{r}\nsummary(cars)\n```\n\nYou can also embed plots, for example:\n\n```{r, echo=FALSE}\nplot(cars)\n```\n\nNote that the `echo = FALSE` parameter was added to the code chunk to prevent printing of the R code that generated the plot.\n",
    "created" : 1418614283158.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "4066637296",
    "id" : "66D99776",
    "lastKnownWriteTime" : 1419203333,
    "path" : "~/Documents/data/coursera/pml/project.Rmd",
    "project_path" : "project.Rmd",
    "properties" : {
        "tempName" : "Untitled1"
    },
    "source_on_save" : false,
    "type" : "r_markdown"
}